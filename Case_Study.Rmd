---
title: "Case Study: Boston Housing Dataset"
author: "Emma Bennett, Pallavi, Lily"
date: 'UC3M, 2025'
output:
  html_document: 
    css: my-theme.css
    theme: cerulean
    highlight: tango
    number_sections: no
    toc: no
    toc_depth: 1
  pdf_document:
    css: my-theme.css
    theme: cerulean
    highlight: tango
    number_sections: yes
    toc: yes
    toc_depth: 1
editor_options:
  chunk_output_type: console
---
```{r}
# Importing libraries
library(mlbench)
library(tidyverse)
library(tibble)
library(knitr)
library(ggcorrplot)
library(mapview)
library(sf)
library(GGally)
library(factoextra)
```

## Information about dataset
According to the description it consists of housing data for 506 census tracts of Boston from the 1970 census. 

This dataset that we chose is good for a lot of these dimensionality reduction techniques (like factor analysis and PCA) because it has a good number of quantitative variables and is well-structured and easy to visualize.

The following are the columns of the dataset:

- **CRIM** — per capita crime rate by town  
- **ZN** — proportion of residential land zoned for lots over 25,000 sq.ft.  
- **INDUS** — proportion of non-retail business acres per town  
- **CHAS** — Charles River dummy variable (1 if tract bounds river; 0 otherwise)  
- **NOX** — nitric oxides concentration (parts per 10 million)  
- **RM** — average number of rooms per dwelling  
- **AGE** — proportion of owner-occupied units built prior to 1940  
- **DIS** — weighted distances to five Boston employment centres  
- **RAD** — index of accessibility to radial highways  
- **TAX** — full-value property-tax rate per \$10,000  
- **PTRATIO** — pupil-teacher ratio by town  
- **B** — 1000(Bk − 0.63)² where Bk is the proportion of blacks by town  
- **LSTAT** — % lower status of the population  
- **MEDV** — median value of owner-occupied homes in \$1000’s  
- **TOWN** — name of the town  
- **TRACT** — census tract number within the town  
- **LON** — longitude coordinate of the census tract centroid  
- **LAT** — latitude coordinate of the census tract centroid  
- **CMEDV** — corrected median value of owner-occupied homes (in \$1000’s, accounting for inflation and updates)

## Loading data

```{r}
data("BostonHousing", package = "mlbench")
data("BostonHousing2", package = "mlbench")
summary(BostonHousing2)
```

```{r}
glimpse(BostonHousing2)
```
```{r}
boxplot(BostonHousing2)
```

### Overlaying Map to Boston Housing
```{python}

# import numpy as np
# import pandas as pd
# 
# # Assuming df = BostonHousing2 loaded as pandas DataFrame
# # and town_names, lat_long = reference coordinates of town centers
# 
# lat_long_values = []
# 
# # Calculate the per-town mean lat/lon
# wrong_values = df.groupby('TOWN')[['LAT', 'LON']].mean()
# 
# # Iterate through each town and its reference point
# for name, (lat, lon) in zip(town_names, lat_long):
#     df_temp = df[df.TOWN == name]
#     mean_temp = wrong_values.loc[name].values  # [mean_lat, mean_lon]
#     
#     for lat_local, lon_local in df_temp[['LAT', 'LON']].values:
#         # difference from mean position
#         diff_lat = lat_local - mean_temp[0]
#         diff_lon = lon_local - mean_temp[1]
#         
#         # translated position relative to reference point
#         lat_long_values.append([lat + diff_lat, lon + diff_lon])
# 
# # Replace in dataframe
# df = df.sort_values('TOWN').copy()
# df[['LAT', 'LON']] = np.asarray(lat_long_values)
# 
# # Rename to lowercase for mapview compatibility
# df.rename(columns={'LAT': 'lat', 'LON': 'lon'}, inplace=True)
# 
# # Export to R if needed:
# df.to_csv("BostonHousing2_corrected.csv", index=False)

```


```{r}
library(mapview)
#BostonHousing2 <- read.csv("BostonHousing2_corrected.csv")

#mapview(BostonHousing2, xcol = "lon", ycol = "lat", grid = TRUE, crs = 4326)
```



## Data preprocessing
Below we are going to do some preprocessing on the data including dealing with outliers, checking if there are any NA values, and other feature engineering.

### Outliers

Here, we are identifying outliers (extreme values) in the Boston Housing dataset.

```{r}
# Example: Corrected median value of owner-occupied homes (cmedv)
BostonHousing2 %>%
  ggplot(aes(y = cmedv)) + 
  geom_boxplot(fill = "lightblue", color = "blue", 
               outlier.color = "red", outlier.shape = 16) + 
  theme_minimal() +
  labs(title = "Boxplot of Median Home Value (cmedv)",
       y = "Median Value ($1000s)")

# Identification by the 3-sigma rule
mu <- mean(BostonHousing2$cmedv, na.rm = TRUE)
sigma <- sd(BostonHousing2$cmedv, na.rm = TRUE)

sum(BostonHousing2$cmedv < mu - 3*sigma | BostonHousing2$cmedv > mu + 3*sigma)

# Identification by the IQR method
QI <- quantile(BostonHousing2$cmedv, 0.25, na.rm = TRUE)
QS <- quantile(BostonHousing2$cmedv, 0.75, na.rm = TRUE)
IQR <- QS - QI

sum(BostonHousing2$cmedv < QI - 1.5*IQR | BostonHousing2$cmedv > QS + 1.5*IQR)

# Depending on the context, we must decide what to do:
# - Remove them if they are errors or impossible values
# - Keep them if they represent true extreme cases

# Extend analysis to all numeric variables
numeric_vars <- BostonHousing2 %>% select(where(is.numeric))

numeric_vars %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value") %>%
  ggplot(aes(x = variable, y = value, fill = variable)) +
  geom_boxplot(outlier.color = "red", outlier.shape = 16) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Boxplots of Numeric Variables in BostonHousing2",
       x = "Variable",
       y = "Value")
```


### NA/Missing Values
In this dataset there are no missing values.
```{r}
barplot(colMeans(is.na(BostonHousing2)), las=2)
```

To show how we would deal with missing values going to remove NA values on a different dataset.


### Feature Engineering


## Visualization Insights

## Regression models and resampling tools

## PCA


```{r}
pca=prcomp(BostonHousing2[,-c(1, 2, 3, 4, 5, 10)], scale=T)
new_data=scale(BostonHousing2[,-c(1, 2, 3, 4, 5, 10)])
boxplot(new_data)
```

```{r}
pca = prcomp(new_data, scale=TRUE)
su = summary(pca)

# calculate mean of variances explained by everything
mean(su$importance[2, ])

# Keep all of the columns that have more than 7% (or whatever that mean val is above) of variance

barplot(pca$rotation[,1])

```


```{r}
ggcorrplot(cor(new_data),lab=T, type="lower")
plot(BostonHousing2$lon,BostonHousing2$lat)
```


```{r}
# Function to determine 
fviz_contrib(pca, choice="var", axes=1)
barplot(pca$rotation[,1])

fviz_contrib(pca, choice="var", axes=2)
barplot(pca$rotation[,2])
# Could interpret this for example as the fact that if distance from city center is low then the median house value is higher

fviz_contrib(pca, choice="var", axes=3)
barplot(pca$rotation[,3])
# Could interpret relationship between highway distance and taxes being inversely proportional for example

```


## Factor Analysis

```{r}
x.f <- factanal(new_data,factors = 3, rotation="none", scores = "regression")
x.f
```

We obtain that factor is same proportion as principal component anlalysis. For PC3 its around 72% and factor analysis is around 66%. 

Can compare positive and negatives between the factor analysis loadings and the PCA loadings.

Interpretation is EXACTLY THE SAME between factor analysis and PCA for factor1.
For factor 2 negatives and positives are swapped for some of them, but all of the relationships are the same so also same as the PCA for dim 2.

```{r}
# Exactly the same as above but now just considering two factors
x.f <- factanal(new_data,factors = 2, rotation="none", scores = "regression")
x.f
```

Factor1 explains same as PCA1. However, if you look at the second factor now it's not the same as PCA2 (because the negatives/positives are messed up).

```{r}
# Could also try different type of rotation (e.g. varimax)
x.f <- factanal(new_data,factors = 3, rotation="varimax", scores = "regression")
x.f
```


## Clustering tools

```{r}
#hclust(X, )
```


